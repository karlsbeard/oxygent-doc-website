---
title: HttpLLM API 参考
description: HttpLLM 类的完整 API 参考文档
icon: Code
---

## 类定义

```python
from oxygent import oxy

llm = oxy.HttpLLM(
    name="my_llm",
    api_key="your_api_key",
    base_url="https://api.openai.com/v1",
    model_name="gpt-4o",
    llm_params={"temperature": 0.7},
    is_multimodal_supported=False,
    timeout=300,
)
```

## 构造函数参数

### 必需参数

|参数|类型|必需|默认值|描述|
|-----------|------|----------|---------|-------------|
| `name` | `str` | **是** | - | MAS 系统中此 LLM 的唯一标识符 |
| `api_key` | `str` | **条件性** | `None` | 用于身份验证的 API 密钥。OpenAI/Gemini 必需，Ollama 不需要 |
| `base_url` | `str` | **是** | - | LLM API 端点的基础 URL（例如 "https://api.openai.com/v1"） |
| `model_name` | `str` | **是** | - | 模型标识符（例如 "gpt-4o"、"gemini-pro"、"llama3.2"） |

### LLM 配置

|参数|类型|默认值|描述|
|-----------|------|---------|-------------|
| `llm_params` | `dict` | `{}` | LLM 调用的默认参数（temperature、max_tokens、top_p 等）。这些参数会随每个请求发送，可以在每次调用时覆盖 |
| `timeout` | `float` | `300` | 最大执行时间（秒）（默认：5 分钟） |
| `category` | `str` | `"llm"` | 类别分类（自动设置，请勿修改） |

### 多模态支持

|参数|类型|默认值|描述|
|-----------|------|---------|-------------|
| `is_multimodal_supported` | `bool` | `False` | 此 LLM 是否支持多模态输入（图像、视频、文档） |
| `is_convert_url_to_base64` | `bool` | `False` | 发送前将媒体 URL 转换为 base64（某些不支持 URL 的 API 需要） |
| `max_image_pixels` | `int` | `10000000` | 最大图像大小（像素）（1000 万）。超过此大小的图像将被拒绝 |
| `max_video_size` | `int` | `12582912` | 最大视频文件大小（字节）（默认：12MB） |
| `max_file_size_bytes` | `int` | `2097152` | base64 嵌入的最大非媒体文件大小（字节）（默认：2MB） |

### HTTP 配置

|参数|类型|默认值|描述|
|-----------|------|---------|-------------|
| `headers` | `dict \| Callable[[OxyRequest], dict]` | `{}` | 随请求发送的额外 HTTP 头。可以是静态字典或接收 OxyRequest 并返回头的函数 |

**Headers Function Signature:**
```python
def get_headers(oxy_request: OxyRequest) -> dict[str, str]:
    return {"X-Custom-Header": "value"}
```

### 消息传递与错误处理

|参数|类型|默认值|描述|
|-----------|------|---------|-------------|
| `is_send_think` | `bool` | 来自 `Config` | 是否提取并向前端发送思考过程消息。支持 `<think>...</think>` XML 标签和 JSON `"think"` 字段 |
| `friendly_error_text` | `str` | `"抱歉，我似乎遇到了问题。请重试。"` | 发生异常时显示的用户友好错误消息 |

### 执行控制

|参数|类型|默认值|描述|
|-----------|------|---------|-------------|
| `semaphore` | `int` | `16` | 允许的最大并发 LLM 请求数 |
| `retries` | `int` | `2` | 失败时的重试次数 |
| `delay` | `float` | `1.0` | 重试尝试之间的延迟（秒） |

### 数据与日志

|参数|类型|默认值|描述|
|-----------|------|---------|-------------|
| `desc` | `str` | `""` | 此 LLM 的可读描述 |
| `is_save_data` | `bool` | `True` | 是否将执行跟踪保存到 Elasticsearch |
| `is_permission_required` | `bool` | `False` | 是否需要权限检查（LLM 通常为 False） |

## 调用参数

通过 `oxy_request.call()` 或作为代理的 `llm_model` 调用 HttpLLM 时，可以传递以下参数：

### 必需参数

```python
response = await oxy_request.call(
    callee="llm_name",
    arguments={
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ]
    }
)
```

|参数|类型|必需|描述|
|----------|------|----------|-------------|
| `messages` | `list[dict]` | **是** | OpenAI 格式的消息对象列表 |

### 消息格式

```python
{
    "role": "system" | "user" | "assistant" | "tool",
    "content": str | list,  # Text or multimodal content
    "name": str,            # Optional: function/tool name
    "tool_calls": list,     # Optional: tool calls made by assistant
    "tool_call_id": str     # Optional: ID of tool call being responded to
}
```

### 多模态消息格式

对于支持视觉的模型：

```python
{
    "role": "user",
    "content": [
        {"type": "text", "text": "What's in this image?"},
        {"type": "image_url", "image_url": {"url": "https://..."}}
    ]
}
```

### 可选参数

|参数|类型|描述|
|----------|------|-------------|
| `temperature` | `float` | 采样温度（0.0 到 2.0） |
| `max_tokens` | `int` | 生成的最大 token 数 |
| `top_p` | `float` | 核采样参数 |
| `frequency_penalty` | `float` | 惩罚频繁 token |
| `presence_penalty` | `float` | 惩罚已出现 token |
| `stop` | `str \| list[str]` | 停止序列 |
| `stream` | `bool` | 启用流式模式 |
| `response_format` | `dict` | 响应格式（例如 `{"type": "json_object"}`)  |

**注意：**可用参数取决于特定的 LLM 提供商。请查阅其文档。

## 方法

### _execute()

向 LLM API 发送 HTTP 请求的内部方法。用户不直接调用。

```python
async def _execute(self, oxy_request: OxyRequest) -> OxyResponse
```

**实现细节：**
- 根据 `base_url` 自动检测提供商（OpenAI、Gemini、Ollama）
- 根据提供商要求格式化请求
- 处理身份验证头
- 支持流式响应
- 处理多模态输入

**Provider Detection Logic:**
```python
if "generativelanguage.googleapis.com" in base_url:
    # Gemini provider
    endpoint = f"{base_url}/models/{model_name}:generateContent"
    auth_header = {"X-goog-api-key": api_key}
elif api_key is not None:
    # OpenAI-compatible provider
    endpoint = f"{base_url}/chat/completions"
    auth_header = {"Authorization": f"Bearer {api_key}"}
else:
    # Ollama (no auth)
    endpoint = f"{base_url}/api/chat"
    auth_header = {}
```

### _get_messages()

处理多模态输入消息的内部方法。不直接调用。

```python
async def _get_messages(self, oxy_request: OxyRequest) -> list[dict]
```

**功能：**
- 解析混合文本/图像/视频内容
- 将 markdown 链接转换为多模态格式
- 内联读取文档文件
- 将 URL 转换为 base64（如果启用）
- 验证图像/视频大小

## 响应格式

### OxyResponse

每个 LLM 调用都返回一个 OxyResponse：

```python
class OxyResponse:
    state: OxyState         # COMPLETED or FAILED
    output: str             # Generated text
    extra: dict             # Metadata (empty for HttpLLM)
    oxy_request: OxyRequest # Echo of request
```

**Example:**
```python
response = await oxy_request.call(
    callee="gpt4",
    arguments={"messages": [...]}
)

if response.state == OxyState.COMPLETED:
    text = response.output  # Generated text from LLM
else:
    error = response.output  # Error message
```

### 流式响应

当 `stream=True` 时，中间消息通过 `send_message()` 发送：

```python
{
    "type": "stream",
    "content": {"delta": "token"},
    "_is_stored": False
}
```

**最终响应：**
```python
OxyResponse(
    state=OxyState.COMPLETED,
    output="complete generated text"  # 所有 token 连接在一起
)
```

## 使用示例

### 基本用法

```python
import os
from oxygent import MAS, oxy

oxy_space = [
    oxy.HttpLLM(
        name="gpt4",
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://api.openai.com/v1",
        model_name="gpt-4o",
        llm_params={"temperature": 0.7}
    ),
    oxy.ChatAgent(
        name="assistant",
        llm_model="gpt4"
    )
]

async def main():
    async with MAS(oxy_space=oxy_space) as mas:
        await mas.start_web_service(first_query="Hello!")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

### 多个提供商

```python
oxy_space = [
    # OpenAI
    oxy.HttpLLM(
        name="gpt4",
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://api.openai.com/v1",
        model_name="gpt-4o"
    ),

    # Gemini
    oxy.HttpLLM(
        name="gemini",
        api_key=os.getenv("GEMINI_API_KEY"),
        base_url="https://generativelanguage.googleapis.com/v1beta",
        model_name="gemini-2.0-flash-exp"
    ),

    # Ollama
    oxy.HttpLLM(
        name="llama",
        api_key=None,
        base_url="http://localhost:11434",
        model_name="llama3.2"
    ),

    # 为不同代理使用不同模型
    oxy.ChatAgent(name="fast", llm_model="gemini"),
    oxy.ReActAgent(name="smart", llm_model="gpt4"),
    oxy.ChatAgent(name="local", llm_model="llama"),
]
```

### 直接 LLM 调用

```python
async def my_workflow(oxy_request: OxyRequest):
    # Call LLM directly
    response = await oxy_request.call(
        callee="gpt4",
        arguments={
            "messages": [
                {"role": "system", "content": "You are a code expert."},
                {"role": "user", "content": "Write a Python function to sort a list."}
            ],
            "temperature": 0.2,
            "max_tokens": 500
        }
    )

    return response.output

oxy_space = [
    oxy.HttpLLM(name="gpt4", ...),
    oxy.WorkflowAgent(
        name="coder",
        func_workflow=my_workflow
    )
]
```

### 动态请求头

```python
def get_auth_headers(oxy_request: OxyRequest) -> dict:
    user_id = oxy_request.get_shared_data("user_id", "anonymous")
    return {
        "X-User-ID": user_id,
        "X-Request-ID": oxy_request.get_request_id()
    }

oxy.HttpLLM(
    name="custom_llm",
    api_key=os.getenv("API_KEY"),
    base_url="https://api.example.com",
    model_name="gpt-4",
    headers=get_auth_headers  # 使用函数而非字典
)
```

### 流式传输

```python
async def streaming_workflow(oxy_request: OxyRequest):
    response = await oxy_request.call(
        callee="gpt4",
        arguments={
            "messages": [
                {"role": "user", "content": "Write a long story."}
            ],
            "stream": True  # Enable streaming
        }
    )
    # Tokens are streamed automatically
    # Final response contains complete text
    return response.output

oxy_space = [
    oxy.HttpLLM(name="gpt4", ...),
    oxy.WorkflowAgent(name="writer", func_workflow=streaming_workflow)
]
```

### 多模态输入

```python
oxy.HttpLLM(
    name="gpt4_vision",
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url="https://api.openai.com/v1",
    model_name="gpt-4o",
    is_multimodal_supported=True,
    is_convert_url_to_base64=False
)

# 使用图像调用
response = await oxy_request.call(
    callee="gpt4_vision",
    arguments={
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Describe this image:"},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": "https://example.com/image.jpg"
                        }
                    }
                ]
            }
        ]
    }
)
```

### 自定义参数

```python
# Set defaults
llm = oxy.HttpLLM(
    name="gpt4",
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url="https://api.openai.com/v1",
    model_name="gpt-4o",
    llm_params={
        "temperature": 0.7,
        "max_tokens": 1000,
        "top_p": 1.0
    }
)

# Override per call
response = await oxy_request.call(
    callee="gpt4",
    arguments={
        "messages": [...],
        "temperature": 0.2,      # Override
        "max_tokens": 500,       # Override
        "frequency_penalty": 0.5 # Add new parameter
    }
)
```

## 提供商特定详情

### OpenAI

**基础 URL：**
- 生产环境：`https://api.openai.com/v1`
- Azure：`https://{resource}.openai.azure.com/openai/deployments/{deployment}`

**模型：**
- `gpt-4o`、`gpt-4-turbo`、`gpt-3.5-turbo`

**身份验证：**`Authorization` 头中的 Bearer token

**端点：** 附加 `/chat/completions`

### Gemini

**基础 URL：**`https://generativelanguage.googleapis.com/v1beta`

**模型：**
- `gemini-2.0-flash-exp`、`gemini-pro`、`gemini-pro-vision`

**身份验证：** `X-goog-api-key` 头中的 API 密钥

**端点：**附加 `/models/{model_name}:generateContent`

**注意：** Gemini 内部使用不同的消息格式（自动转换）

### Ollama

**基础 URL：** `http://localhost:11434`（默认）

**模型：** 本地安装的任何模型（llama3.2、mistral 等）

**身份验证：**无需

**端点：** 附加 `/api/chat`

**注意：** 无需 API 密钥，本地运行

## 错误处理

### 常见错误

|错误类型|原因|解决方案|
|------------|-------|----------|
| `ValueError: base_url must be a non-empty string` | 缺少或空的 base_url | 设置有效的 base_url |
| `ValueError: model_name must be a non-empty string` | 缺少 model_name | 设置有效的 model_name |
| `ValueError: LLM API error: ...` | API 返回错误 | 检查 API 密钥、配额、模型名称 |
| `httpx.TimeoutException` | 请求超时 | 增加 `timeout` 参数 |
| `httpx.HTTPStatusError` | HTTP 错误（4xx、5xx） | 检查 API 凭证和状态 |

### 重试逻辑

HttpLLM 自动重试失败的请求：

```python
oxy.HttpLLM(
    name="resilient_llm",
    api_key=os.getenv("API_KEY"),
    base_url="https://api.example.com",
    model_name="gpt-4",
    retries=3,  # 最多重试 3 次
    delay=2.0,  # 重试之间等待 2 秒
    timeout=120 # 每次尝试 2 分钟超时
)
```

**重试行为：**
- 最多尝试 `retries` 次
- 每次尝试之间等待 `delay` 秒
- 用尽重试后返回 `OxyState.FAILED`

## 相关文档

- **[HttpLLM 指南](/docs/llms-http)** - 主要文档和使用模式
- **[ChatAgent](/docs/agents-chat)** - 使用 LLM 的对话代理
- **[ReActAgent API](/oxyapi/agents-react-api)** - 推理代理 API
- **[配置](/docs/configuration)** - 全局 LLM 配置
- **[OxyRequest 与上下文](/docs/context)** - 理解调用 API
