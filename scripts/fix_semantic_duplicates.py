#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤æ–‡æ¡£ä¸­è¯­ä¹‰ç›¸ä¼¼çš„é‡å¤å†…å®¹
- æ£€æµ‹ frontmatter description ä¸å†…å®¹å¼€å¤´çš„è¯­ä¹‰é‡å¤
- ç§»é™¤å†—ä½™çš„æè¿°æ€§æ®µè½
"""

import re
from pathlib import Path
from typing import Dict, List, Tuple


def parse_frontmatter(content: str) -> Tuple[Dict[str, str], str]:
    """è§£æ frontmatter å’Œå†…å®¹"""
    lines = content.split("\n")

    if not lines[0].strip() == "---":
        return {}, content

    frontmatter = {}
    content_start = 0

    for i, line in enumerate(lines[1:], 1):
        if line.strip() == "---":
            content_start = i + 1
            break

        if ":" in line:
            key, value = line.split(":", 1)
            frontmatter[key.strip()] = value.strip().strip("\"'")

    remaining_content = "\n".join(lines[content_start:])
    return frontmatter, remaining_content


def is_semantic_duplicate(description: str, content_line: str) -> bool:
    """æ£€æŸ¥ä¸¤ä¸ªå¥å­æ˜¯å¦è¯­ä¹‰é‡å¤"""
    if not description or not content_line:
        return False

    # æ ‡å‡†åŒ–æ–‡æœ¬ï¼šè½¬å°å†™ï¼Œç§»é™¤æ ‡ç‚¹ç¬¦å·
    def normalize(text):
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œå¤šä½™ç©ºæ ¼
        text = re.sub(r"[^\w\s]", " ", text.lower())
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r"\s+", " ", text.strip())
        return text

    desc_norm = normalize(description)
    content_norm = normalize(content_line)

    # å¦‚æœå®Œå…¨ç›¸åŒ
    if desc_norm == content_norm:
        return True

    # æ£€æŸ¥å…³é”®è¯é‡å åº¦
    desc_words = set(desc_norm.split())
    content_words = set(content_norm.split())

    # ç§»é™¤å¸¸è§çš„åœç”¨è¯
    stop_words = {
        "a",
        "an",
        "the",
        "and",
        "or",
        "but",
        "in",
        "on",
        "at",
        "to",
        "for",
        "of",
        "with",
        "by",
        "is",
        "are",
        "was",
        "were",
        "be",
        "been",
        "being",
        "have",
        "has",
        "had",
        "do",
        "does",
        "did",
        "will",
        "would",
        "could",
        "should",
        "may",
        "might",
        "can",
        "this",
        "that",
        "these",
        "those",
    }

    desc_words -= stop_words
    content_words -= stop_words

    if not desc_words or not content_words:
        return False

    # è®¡ç®—é‡å åº¦
    overlap = len(desc_words & content_words)
    total_unique = len(desc_words | content_words)
    overlap_ratio = overlap / total_unique if total_unique > 0 else 0

    # å¦‚æœé‡å åº¦è¶…è¿‡70%ï¼Œè®¤ä¸ºæ˜¯è¯­ä¹‰é‡å¤
    if overlap_ratio > 0.7:
        return True

    # ç‰¹æ®Šæ¨¡å¼æ£€æµ‹ï¼šAPI æ–‡æ¡£æè¿°
    api_patterns = [
        (r"complete.*api.*reference", r"complete.*api.*documentation"),
        (r"api.*reference.*for", r"api.*documentation.*for"),
        (r"complete.*reference", r"complete.*documentation"),
    ]

    for pattern1, pattern2 in api_patterns:
        if (re.search(pattern1, desc_norm) and re.search(pattern2, content_norm)) or (
            re.search(pattern2, desc_norm) and re.search(pattern1, content_norm)
        ):
            return True

    return False


def fix_semantic_duplicates(file_path: Path) -> Dict[str, any]:
    """ä¿®å¤å•ä¸ªæ–‡ä»¶çš„è¯­ä¹‰é‡å¤å†…å®¹"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            original_content = f.read()
    except Exception as e:
        return {"error": f"Error reading file: {e}"}

    frontmatter, content = parse_frontmatter(original_content)

    if not frontmatter:
        return {"status": "no_frontmatter", "changes": 0}

    title = frontmatter.get("title", "")
    description = frontmatter.get("description", "")

    if not description:
        return {"status": "no_description", "changes": 0}

    lines = content.split("\n")
    new_lines = []
    changes = 0
    skip_next_empty = False

    i = 0
    while i < len(lines):
        line = lines[i].strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤çš„ # æ ‡é¢˜
        if line.startswith("# ") and line[2:].strip() == title:
            changes += 1
            skip_next_empty = True
            i += 1
            continue

        # æ£€æŸ¥æ˜¯å¦æ˜¯è¯­ä¹‰é‡å¤çš„æè¿°
        if line and is_semantic_duplicate(description, line):
            # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰ç©ºè¡Œï¼Œå¦‚æœæœ‰ä¹Ÿè¦è·³è¿‡
            if new_lines and new_lines[-1].strip() == "":
                new_lines.pop()
            changes += 1
            skip_next_empty = True
            i += 1
            continue

        # è·³è¿‡é‡å¤å†…å®¹åçš„ç¬¬ä¸€ä¸ªç©ºè¡Œ
        if skip_next_empty and line == "":
            skip_next_empty = False
            i += 1
            continue

        skip_next_empty = False
        new_lines.append(lines[i])
        i += 1

    if changes > 0:
        # é‡å»ºå®Œæ•´å†…å®¹
        frontmatter_lines = ["---"]
        for key, value in frontmatter.items():
            frontmatter_lines.append(f"{key}: {value}")
        frontmatter_lines.append("---")
        frontmatter_lines.append("")  # ç©ºè¡Œåˆ†éš”

        new_content = "\n".join(frontmatter_lines + new_lines)

        # å†™å›æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(new_content)

    return {
        "status": "processed",
        "changes": changes,
        "title": title,
        "description": (
            description[:50] + "..." if len(description) > 50 else description
        ),
    }


def main():
    """ä¸»å‡½æ•°"""
    base_dir = Path("/Users/chengkai48/Documents/AI/oxygent-doc-website/content")

    # è¦å¤„ç†çš„ç›®å½•
    directories = [base_dir / "oxyapi", base_dir / "examples", base_dir / "docs"]

    print("=" * 80)
    print("ğŸ”§ Fix Semantic Duplicate Descriptions")
    print("=" * 80)

    total_files = 0
    total_changes = 0
    files_modified = 0

    for directory in directories:
        if not directory.exists():
            print(f"âš ï¸  Directory not found: {directory}")
            continue

        print(f"\nğŸ“ Processing: {directory.name}/")
        print("-" * 60)

        mdx_files = list(directory.glob("*.mdx"))

        for file_path in sorted(mdx_files):
            total_files += 1
            result = fix_semantic_duplicates(file_path)

            if "error" in result:
                print(f"âŒ {file_path.name}: {result['error']}")
            elif result["status"] == "no_frontmatter":
                print(f"âšª {file_path.name}: No frontmatter")
            elif result["status"] == "no_description":
                print(f"âšª {file_path.name}: No description in frontmatter")
            elif result["changes"] == 0:
                print(f"âœ… {file_path.name}: No semantic duplicates found")
            else:
                files_modified += 1
                total_changes += result["changes"]
                print(
                    f"ğŸ”§ {file_path.name}: Fixed {result['changes']} semantic duplicates"
                )
                print(f"   Title: {result['title']}")
                print(f"   Desc:  {result['description']}")

    print("\n" + "=" * 80)
    print("ğŸ“Š Summary:")
    print(f"   - Total files processed: {total_files}")
    print(f"   - Files modified: {files_modified}")
    print(f"   - Total semantic duplicates removed: {total_changes}")

    if files_modified > 0:
        print(f"\nâœ… Successfully fixed semantic duplicates in {files_modified} files!")
    else:
        print(f"\nâœ… No semantic duplicates found!")

    print("=" * 80)


if __name__ == "__main__":
    main()
